{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Liam Byrne\n",
    "##### DATA 620 - Web Analytics\n",
    "##### Fall - 2017\n",
    "\n",
    "# Project 4\n",
    "\n",
    "***\n",
    "\n",
    "## Introduction\n",
    "From `nltk's` movie review corpus, we are given 2000 movie reviews with the binary rating of `positive` or `negative`. The text book ([Section 6.3](http://www.nltk.org/book/ch06.html#ref-document-classify-set)) outlines a feature extraction method:\n",
    "\n",
    "+ Binarize the presence of high frequency words in the reviews\n",
    "+ Train a Naive Bayes classifier on the reviews and rating\n",
    "+ Output the most informative features in the classification method\n",
    "\n",
    "We will replicate this and look at the 30 most informative features in the reviews that predict a rating. We will then try to infer why these features are are informative and make a basic comparison against our predictions and `nltk's` sentiment analyzer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package and Data Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import random\n",
    "import re\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "# Gather reviews from movie_reviews\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "        for category in movie_reviews.categories()\n",
    "        for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# Seed and shuffle docs\n",
    "random.seed(2**7-1)\n",
    "random.shuffle(documents)\n",
    "\n",
    "# Get the frequency distribution of all words and keep the top 2000\n",
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "word_features = list(all_words)[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Definitions\n",
    "The following feature extractor creates a feature set for each review with the binarized presence of the top 2000 words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    \n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction, Splits and Classification\n",
    "Unlike the text, where they only kept a 5% hold-out for the test, we will keep a 20% hold-out to prevent over-fitting. After the classifier is trained, we will look at the most informative features and infer their meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8025\n",
      "Most Informative Features\n",
      "         contains(tones) = True              pos : neg    =      9.0 : 1.0\n",
      "        contains(turkey) = True              neg : pos    =      7.8 : 1.0\n",
      "         contains(groan) = True              neg : pos    =      7.0 : 1.0\n",
      "    contains(schumacher) = True              neg : pos    =      6.6 : 1.0\n",
      "       contains(martian) = True              neg : pos    =      6.3 : 1.0\n",
      "        contains(welles) = True              neg : pos    =      6.3 : 1.0\n",
      "        contains(shoddy) = True              neg : pos    =      6.3 : 1.0\n",
      "      contains(everyday) = True              pos : neg    =      6.2 : 1.0\n",
      "       contains(singers) = True              pos : neg    =      5.7 : 1.0\n",
      "        contains(temper) = True              pos : neg    =      5.7 : 1.0\n",
      "       contains(bronson) = True              neg : pos    =      5.6 : 1.0\n",
      "       contains(miscast) = True              neg : pos    =      5.5 : 1.0\n",
      "         contains(kudos) = True              pos : neg    =      5.4 : 1.0\n",
      "     contains(atrocious) = True              neg : pos    =      5.4 : 1.0\n",
      "        contains(wasted) = True              neg : pos    =      5.2 : 1.0\n",
      "         contains(waste) = True              neg : pos    =      5.2 : 1.0\n",
      "        contains(poorly) = True              neg : pos    =      5.1 : 1.0\n",
      "           contains(ugh) = True              neg : pos    =      5.0 : 1.0\n",
      "     contains(pregnancy) = True              neg : pos    =      5.0 : 1.0\n",
      "     contains(underwood) = True              neg : pos    =      5.0 : 1.0\n",
      "    contains(uninspired) = True              neg : pos    =      4.6 : 1.0\n",
      "        contains(justin) = True              neg : pos    =      4.6 : 1.0\n",
      "       contains(highway) = True              neg : pos    =      4.6 : 1.0\n",
      "         contains(awful) = True              neg : pos    =      4.4 : 1.0\n",
      "     contains(paralyzed) = True              pos : neg    =      4.4 : 1.0\n",
      "       contains(unravel) = True              pos : neg    =      4.4 : 1.0\n",
      "       contains(seymour) = True              pos : neg    =      4.4 : 1.0\n",
      "        contains(sorrow) = True              pos : neg    =      4.4 : 1.0\n",
      "        contains(canyon) = True              neg : pos    =      4.3 : 1.0\n",
      "   contains(marketplace) = True              neg : pos    =      4.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "\n",
    "# Change split to 20%  hold-out (from 5%) due to overfitting\n",
    "train_set, test_set = featuresets[int(len(featuresets)*.2):], \\\n",
    "                      featuresets[:int(len(featuresets)*.2)]\n",
    "                        \n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# Print accuracy score of classified test data\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "\n",
    "# Show the top 30 most informative features\n",
    "classifier.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts on Informative Features\n",
    "An ~80% accuracy is very promising, using just the presence of highly frequent words in the reviews. Looking at the list of features, it may not be easy to see that the use of these words in reviews should have a correlation to the rating of the movie. For instance, the use of the word `tones` has a positive negative ratio (p/n) of 9:1. `tones` could relate to the themes of the movie or allusions to different movies. It shows that the writer may be passionate about the movie to go into such detail as remarking on the `tone`. Similarly, `groan` having a p/s of 1:7 is most likely the writer describing their reaction to the poor quality of the movie. There are also terms that seem to be directors'/actors'/actresses' names (e.g.: `welles`, `schumacher` and `underwood`). These could relate to the quality of the movies that these people are in or the cliched copying of their styles.\n",
    "\n",
    "There are some unusual appearances in the top of the list, namely `turkey` with a p/n of 1:7.8. It is not known what context this is used in; maybe Turkish films are notoriously bad or this is movie review speak for a bad movie?\n",
    "\n",
    "Simply put, these features should elicit some sort of emotional response. Even the noun features like `highway`, `canyon` and `marketplace` elicit some flash, feeling or idea when one uses it in writing. `nltk` has a sentiment analysis module that it borrows from VADER (Valence Aware Dictionary and sEntiment Reasoner) Sentiment Analysis. According to the [repository](https://github.com/cjhutto/vaderSentiment), \"VADER is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media, and works well on texts from other domains.\" The following will compare our feature set reviews to the results in VADER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Select Feature Prediction to VADER\n",
    "![\"Vader\"](http://t-redactyl.io/figure/Vader_1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated previously, VADER was designed to analyze social media. For our data, i.e. a single words extracted from movie reviews, we only have one word to use and as we will see, VADER does not do a good job with single words; as it is expected to. \n",
    "\n",
    "For context, you need sentence structure and all the grammar that goes into describing a sentiment. VADER will see most singular words as neutral, unless they have a strong enough sentiment by themselves. The following creates a table to compare the high scoring features and their resultant `positive` or `negative` rating against a `positive`, `negative` or `neutral` sentiment from VADER.\n",
    "\n",
    "The table features are as follows:\n",
    "+ word: The word from the movie review with the most informative features\n",
    "+ rating: The movie review, with `pos: positive` and `neg: negative`\n",
    "+ pos: The positive sentiment score for the word from VADER\n",
    "+ neg: The negative sentiment score for the word from VADER\n",
    "+ neu: The neutral sentiment score for the word from VADER\n",
    "+ compound: sum of all of the lexicon ratings,  which have been standardized to range between -1 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>rating</th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tones</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>turkey</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>groan</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>schumacher</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>martian</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>welles</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>shoddy</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>everyday</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>singers</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>temper</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.4215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>bronson</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>miscast</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>kudos</td>\n",
       "      <td>pos</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>atrocious</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>wasted</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.4939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>waste</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.4215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>poorly</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ugh</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.4215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>pregnancy</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>underwood</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>uninspired</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>justin</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>highway</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>awful</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.4588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>paralyzed</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>unravel</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>seymour</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>sorrow</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.5267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>canyon</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>marketplace</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word rating  pos  neg  neu  compound\n",
       "0         tones    pos  0.0  0.0  1.0    0.0000\n",
       "1        turkey    neg  0.0  0.0  1.0    0.0000\n",
       "2         groan    neg  0.0  0.0  1.0    0.0000\n",
       "3    schumacher    neg  0.0  0.0  1.0    0.0000\n",
       "4       martian    neg  0.0  0.0  1.0    0.0000\n",
       "5        welles    neg  0.0  0.0  1.0    0.0000\n",
       "6        shoddy    neg  0.0  0.0  1.0    0.0000\n",
       "7      everyday    pos  0.0  0.0  1.0    0.0000\n",
       "8       singers    pos  0.0  0.0  1.0    0.0000\n",
       "9        temper    pos  0.0  1.0  0.0   -0.4215\n",
       "10      bronson    neg  0.0  0.0  1.0    0.0000\n",
       "11      miscast    neg  0.0  0.0  1.0    0.0000\n",
       "12        kudos    pos  1.0  0.0  0.0    0.5106\n",
       "13    atrocious    neg  0.0  0.0  1.0    0.0000\n",
       "14       wasted    neg  0.0  1.0  0.0   -0.4939\n",
       "15        waste    neg  0.0  1.0  0.0   -0.4215\n",
       "16       poorly    neg  0.0  0.0  1.0    0.0000\n",
       "17          ugh    neg  0.0  1.0  0.0   -0.4215\n",
       "18    pregnancy    neg  0.0  0.0  1.0    0.0000\n",
       "19    underwood    neg  0.0  0.0  1.0    0.0000\n",
       "20   uninspired    neg  0.0  0.0  1.0    0.0000\n",
       "21       justin    neg  0.0  0.0  1.0    0.0000\n",
       "22      highway    neg  0.0  0.0  1.0    0.0000\n",
       "23        awful    neg  0.0  1.0  0.0   -0.4588\n",
       "24    paralyzed    pos  0.0  0.0  1.0    0.0000\n",
       "25      unravel    pos  0.0  0.0  1.0    0.0000\n",
       "26      seymour    pos  0.0  0.0  1.0    0.0000\n",
       "27       sorrow    pos  0.0  1.0  0.0   -0.5267\n",
       "28       canyon    neg  0.0  0.0  1.0    0.0000\n",
       "29  marketplace    neg  0.0  0.0  1.0    0.0000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instantitate Sentiment Analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Collect the 30 most informative features\n",
    "feat30 = classifier.most_informative_features(30)\n",
    "feat30 = [{w:b} for w,b in feat30]\n",
    "\n",
    "# Create container of word and the classified rating (pos/neg)\n",
    "feat_sent = [{\"word\": re.sub(\"^.*\\(|\\).*$\", \"\", list(w)[0]),\n",
    "              \"rating\": classifier.classify(w)} for w in feat30]\n",
    "\n",
    "# Get sentiment scores for words\n",
    "for feat in feat_sent:\n",
    "    feat.update(sid.polarity_scores(feat[\"word\"]))\n",
    "\n",
    "feat_sent_df = pd.DataFrame(feat_sent)\n",
    "cols = ['word', 'rating', 'pos', 'neg', 'neu', 'compound']\n",
    "feat_sent_df = feat_sent_df[cols]\n",
    "\n",
    "display(feat_sent_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the 20 reviews, VADER only ventures a non-neutral guess 7/20 times. When it does, it is correct 5/7 times, which is ~25% accuracy relative to the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "The simple Naive Bayes classifier did a fairly decent job at classifying the reviews as being positive or negative. Most of the top scoring features were not that intuitive at face value; one would be expecting words like \"excellent\", \"awesome\" or \"best\" to be top positive review predictors and words like \"worst\", \"terrible\" or \"garbage\" to be top negative review predictors. Movie reviewers may dance around these words as they sort of signal an unfair and poor review. These words need to take meaning in descriptive phrases and context, which VADER seemed to need in order to classify a statement as positive or negative.\n",
    "\n",
    "The use of VADER was an off tangent idea to use after realizing that the feature words were most likely sentiment based. It just shows the complexity of classification and how it can not be a one size fits all application. With the complexity of language and the nuance of words, a much more robust method must be used to classify even movie reviews with a high degree of accuracy."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
