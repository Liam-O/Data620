{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Liam Byrne\n",
    "##### DATA 620 - Web Analytics\n",
    "##### Fall - 2017\n",
    "\n",
    "# Project 4\n",
    "\n",
    "***\n",
    "\n",
    "## Introduction\n",
    "Sentiment analysis Classification of documents can have many uses; from isolating documents of interest from large corpora to determining origins of texts. This exercise will explore an elementary step in text classification; an email spam filter.\n",
    "\n",
    "Using the [Spambase Data Set](http://archive.ics.uci.edu/ml/datasets/Spambase) from UCI's Machine Learning Repository, we will built, train and test a few classification models. The data set consists of extracted features (56 features covering things like specific word frequencies, punctuation frequencies and statistics on capital letters) and the emails labeled as either \"spam\" or \"ham\" (non-spam emails).\n",
    "\n",
    "We will first load in our data and import the required packages from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import random\n",
    "import re\n",
    "\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "        for category in movie_reviews.categories()\n",
    "        for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "random.seed(7)\n",
    "random.shuffle(documents)\n",
    "\n",
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "word_features = list(all_words)[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    \n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8125\n",
      "Most Informative Features\n",
      "           contains(ugh) = True              neg : pos    =      8.0 : 1.0\n",
      " contains(unimaginative) = True              neg : pos    =      7.3 : 1.0\n",
      "     contains(atrocious) = True              neg : pos    =      6.9 : 1.0\n",
      "        contains(shoddy) = True              neg : pos    =      6.6 : 1.0\n",
      "    contains(schumacher) = True              neg : pos    =      6.5 : 1.0\n",
      "         contains(waste) = True              neg : pos    =      5.9 : 1.0\n",
      "      contains(everyday) = True              pos : neg    =      5.9 : 1.0\n",
      "        contains(turkey) = True              neg : pos    =      5.8 : 1.0\n",
      " contains(technological) = True              pos : neg    =      5.4 : 1.0\n",
      "          contains(mena) = True              neg : pos    =      5.2 : 1.0\n",
      "        contains(suvari) = True              neg : pos    =      5.2 : 1.0\n",
      "        contains(canyon) = True              neg : pos    =      5.2 : 1.0\n",
      "     contains(underwood) = True              neg : pos    =      5.2 : 1.0\n",
      "         contains(awful) = True              neg : pos    =      4.8 : 1.0\n",
      "       contains(unravel) = True              pos : neg    =      4.8 : 1.0\n",
      "        contains(poorly) = True              neg : pos    =      4.8 : 1.0\n",
      "         contains(snake) = True              neg : pos    =      4.6 : 1.0\n",
      "      contains(explores) = True              pos : neg    =      4.5 : 1.0\n",
      "    contains(ridiculous) = True              neg : pos    =      4.5 : 1.0\n",
      "     contains(entendres) = True              neg : pos    =      4.5 : 1.0\n",
      "  contains(surveillance) = True              neg : pos    =      4.5 : 1.0\n",
      "       contains(bronson) = True              neg : pos    =      4.5 : 1.0\n",
      "     contains(pregnancy) = True              neg : pos    =      4.5 : 1.0\n",
      "   contains(marketplace) = True              neg : pos    =      4.5 : 1.0\n",
      "       contains(runtime) = True              neg : pos    =      4.5 : 1.0\n",
      "        contains(welles) = True              neg : pos    =      4.4 : 1.0\n",
      "         contains(climb) = True              neg : pos    =      4.4 : 1.0\n",
      "     contains(stretched) = True              neg : pos    =      4.3 : 1.0\n",
      "       contains(unfunny) = True              neg : pos    =      4.3 : 1.0\n",
      "        contains(stupid) = True              neg : pos    =      4.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "\n",
    "# Change split to 20%  hold-out (from 5%) due to overfitting\n",
    "train_set, test_set = featuresets[int(len(featuresets)*.2):], \\\n",
    "                      featuresets[:int(len(featuresets)*.2)]\n",
    "                        \n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# Score is different than that in text (0.81) --> Bad random_shuffle?\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "\n",
    "classifier.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantitate Sentiment Analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Collect the 30 most informative features\n",
    "feat30 = classifier.most_informative_features(30)\n",
    "feat30 = [{w:b} for w,b in feat30]\n",
    "\n",
    "# Create container of word and the classified rating (pos/neg)\n",
    "feat_sent = [{\"word\": re.sub(\"^.*\\(|\\).*$\", \"\", w.keys()[0]),\n",
    "              \"rating\": classifier.classify(w)} for w in feat30]\n",
    "\n",
    "# Get sentiment scores for words\n",
    "for feat in feat_sent:\n",
    "    feat.update(sid.polarity_scores(feat[\"word\"]))\n",
    "\n",
    "feat_sent_df = pd.DataFrame(feat_sent)\n",
    "cols = ['word', 'rating', 'pos', 'neg', 'neu', 'compound']\n",
    "feat_sent_df = feat_sent_df[cols]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
